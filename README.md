# decision-trees

# Задание
Основные цели этого задания:

Закрепить теорию построения сплитов в дереве решений.

Изучить и реализовать алгоритм построения дерева.

Разобраться, как работают параметры .

Задача:

Построить модель машинного обучения, предсказывающую траты у посетителей магазина.

План решения:

Загрузите данные и подготовьте их:

Сделайте из текстового признака, обозначающего пол, бинарный флаг 0/1

Переименуйте признаки так, чтобы в названиях не было пробелов.

Доработайте функцию построения сплита (разработанную в рамках скринкаста):

Занесите расчет хаотичности целевого признака в выборке в саму функцию построения сплита

Уберите промежуточные выводы

В качестве результата отработки функции возвращайте пару “признак”+”значение для сплита”, при которых достигается наилучший прирост информации. Если таких не нашлось (если выборка однородна с точки зрения всех признаков и ее невозможно разделить), возвращайте, например, None.

Сделаем функцию похожей на библиотечную: пусть она принимает на вход отдельно датафрейм с признаками (назовите параметр X ) и столбец со значениями целевого признака (назовите y). Доработайте код функции под новые входные данные. Подсказка: чтобы понять, какие значения целевого признака будут принимать объекты, попавшие в левую/правую часть подвыборки, создайте маску из столбца с фичей и примените ее к столбца с целевым признаком:

Чтобы получить маску, к столбцу в признаком, по которому происходит деление примените операцию сравнения со значением, по которому хотите выполнить сплит. Получится столбец из True/False длины, равной количеству элементов в исходной выборке, подлежащей сплиту.

Примените полученную маску к столбцу с целевыми значениями target[mask]. В итоге останутся только значения целевой переменной для наблюдений, которым в маске соответствовало значение True.

Реализуйте функцию построения решающего дерева fit_regression_tree, допускающую ограничения на максимальную глубину дерева, а также минимальное число наблюдений в выборке, подлежащей сплиту. В результате запуска функции должны быть выведены правила, по которым должен происходить сплит, а в листьях - предикт (то есть среднее значение целевого признака на подвыборке обучающего датафйрема, попавшей в этот лист). Бинарное дерево строится рекурсивной, поэтому функция его простроения тоже будет рекурсивной:

Реализуйте часть для выхода из рекурсии. Здесь должна выполняться проверка на достижение максимально допустимой глубины дерева/достаточное количество наблюдений в выборке для выполнения очередного сплита/возможность построения сплита. Невозможно построить сплит, если все объекты в выборке одинаковы с точки зрения значений фичей. Если одно из этих условий выполняется, выводите предикт в следующем формате:
"| " * <текущая глубина дерева>"|---" value: <предикт с 2 знаками после запятой>]'

Реализуйте альтернативный путь для дальнейшего построения дерева:

Выведите правило, по которому наблюдения “идут влево” в формате:
"| " * <текущая глубина дерева>"|---" <фича для сплита> <= <значение для сплита>

Постройте сплит “пошедшей влево” подвыборки

Выведите правило, по которому наблюдения “идут вправо” в формате:
"| " * <текущая глубина дерева>"|---" <фича для сплита> > <значение для сплита>

Постройте сплит “пошедшей вправо” подвыборки.

Проверьте корректность реализованного алгоритма

Убедитесь, что без ошибок строится дерево без ограничений (то есть до последнего возможного сплита)

Убедитесь, что без ошибок отрабатывает алгоритм для построения дерева с ограничением на глубину. Выберите максимально допустимую глубину, равную 2. Также постройте дерево с помощью библиотечного алгоритма (DecisionTreeRegressor) на тех же входных данных. Выведите его с помощью функции export_text и сравните построенные деревья. Они должны совпадать.

Убедитесь, что без ошибок отрабатывает алгоритм для построения дерева с ограничением на минимальное количество элементов в выборке, над которой производится в сплит. Также сравните построенное дерево с деревом, построенным с помощью библиотечной реализации, при ограничении min_samples_split=70.
